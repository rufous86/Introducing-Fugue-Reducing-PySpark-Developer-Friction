# Знакомство с Fugue - меньше сложностей при разработке PySpark

*Повышение производительности разработчиков и снижение затрат на проекты Big Data*

![title_photo.png](assets/title_photo.png)

## Мотивация

Специалисты по обработке данных часто начинают работать с Pandas или SQL. Рано или поздно объем обрабатываемых данных превосходит возможности Pandas, и возникает необходимость в распределенных вычислениях. Одним из таких инструментов является Spark, популярная система распределенных вычислений, которая позволяет обрабатывать большие объемы данных в памяти на кластере машин. Хотя механизм Spark очень мощный для масштабирования конвейеров данных, существует множество подводных камней, с которыми сталкиваются новички и даже опытные пользователи при использовании Spark.

Вполне ожидаемо, что первоначальная трудность заключается в необходимости изучения совершенно нового фреймворка. Синтаксис и использование Spark и Pandas сильно отличаются. Пользователи, переносящие проекты с Pandas на Spark, часто сталкиваются с необходимостью переписать большую часть кода, даже для той же самой логики приложения. Хуже того, некоторые операции, которые в Pandas выполняются тривиально, в Spark становятся намного сложнее и требуют много времени для реализации.

**Простой пример подобной операции - получение медианы для каждой группы данных.** В Pandas нет необходимости думать дважды о получении медианы для каждой группы. Однако в Spark эта операция не так проста. Мы сравним синтаксис двух фреймворков в приведенном ниже фрагменте кода:

```python
# Pandas
df.groupby("col1")["col2"].median()

# PySpark
from pyspark.sql import Window
import pyspark.sql.functions as F

med_func = F.expr('percentile_approx(col2, 0.5, 20)')
df.groupBy('col1').agg(med_func).show()
```
Такое несоответствие синтаксиса объясняется тем, что вычисление медианы в распределенной среде требует больших затрат. Все данные, принадлежащие одной группе, должны быть перенесены на одну машину. Таким образом, перед получением медианы данные необходимо перетасовать и отсортировать. Чтобы снизить затраты на вычисления, можно получить приблизительную медиану с заданным допуском. В приведенном фрагменте 20 - это точность, то есть относительная ошибка может составлять 1/20, или 5%. Уточнение допуска позволяет пользователям найти баланс между точностью и скоростью.

Помимо разницы в синтаксисе, **в распределенной среде существуют важные понятия (такие как разделение, перемешивание, сохранение и ленивые вычисления)**, о которых пользователи Pandas изначально не знают. Эти концепции требуют значительного времени для изучения и освоения, что затрудняет полноценное использование движка Spark.

[Fugue](https://github.com/fugue-project/fugue), слой абстракции с открытым исходным кодом, обеспечивает бесшовный переход от окружения с одной машиной к вычислениями в распределенной среде. С помощью Fugue пользователи могут кодировать свою логику на родном языке Python, Pandas или SQL, а затем передавать ее в движок Spark (или Dask) для исполнения. Это означает, что **пользователям даже не нужно изучать синтаксис Spark, чтобы использовать Spark.**

![fugue_logo.png](assets/fugue_logo.png)

В этой статье мы обсудим болевые точки, с которыми сталкиваются пользователи Spark, и то, как Fugue решает эти проблемы. **Fugue - это прямой результат нескольких лет размышлений о том, как можно улучшить работу разработчиков Spark.** Помимо обеспечения более простого интерфейса для кодирования в Spark, есть и более ощутимые преимущества, которые дает использование слоя абстракции. Здесь мы покажем, как Fugue:

- Справляется с несовместимостью между различными вычислительными фреймворками (Pandas, Spark и Dask)
- Обеспечивает возможность повторного использования кода как для данных, помещающихся в Pandas, так и для данных, для которых необходимо использование Spark
- Значительно ускоряет тестирование и снижает общую стоимость проекта
- Позволяет новым пользователям гораздо быстрее начать продуктивную работу с Spark
- Предоставляет SQL-интерфейс, способный обрабатывать полные рабочие процессы от начала до конца

## Несовместимость между Pandas и Spark
*Можем ли быть единый интерфейс для больших и малых данных?*

Пользователи Pandas, переходящие на Spark, часто сталкиваются с противоречивым поведением. Во-первых, Pandas допускает смешанные типы столбцов. Это означает, что строки и числа могут быть смешаны в одном столбце. В Spark schema строго соблюдается, и столбцы смешанного типа не допускаются. Это связано с тем, что Pandas обладает роскошью видеть все данные в процессе выполнения операций, в то время как Spark выполняет операции на нескольких машинах, на которых хранятся различные части данных. Это означает, что Spark может легко заставить разные разделы вести себя по-разному, если schema не будет строго соблюдаться.

Значения NULL также обрабатываются по-разному в Pandas и Spark. В таблице ниже приведена сводная информация об обработке значений NULL по умолчанию

![null_handling.png](assets/null_handling.png)

Это первое преимущество использования Fugue в качестве слоя абстракции. Одно дело получить код Pandas для работы на Spark, однако написание кода, дающего согласованные результаты между разными инструментами, это очень утомительный процесс. Во многих случаях для получения одинаковых результатов приходится писать дополнительный код. Fugue заботится о согласованности, чтобы создать мост между Pandas и Spark. Fugue был разработан для совместимости со Spark и SQL, поскольку это гарантирует, что код будет работать так, как и ожидается. **Пользователи не должны тратить свое время, заботясь о поведении, специфичном для конкретного фреймворка.**

## Развязка логики и исполнения
*Почему я должен выбирать фреймворк прежде, чем начинать проект по работе с данными?*

Одна из проблем при использовании Pandas и Spark заключается в том, что логика тесно связана с интерфейсом. Это непрактично, поскольку требует  от специалистов по работе с данными выбора, с чем они будут работать, уже на начальном этапе проекта. Вот два сценария, которые представляют две стороны одной и той же проблемы.
1. Пользователь пишет код в Pandas, а затем данные становятся слишком большими. Чтобы решить эту проблему, необходимо модернизировать базовое оборудование для поддержки исполнения кода (вертикальное масштабирование).
2. Пользователь кодирует в Spark, ожидая, что данные будут большими, но они никогда не вырастают до размера, требующего Spark. Код и тесты работают медленнее, чем должны, из-за накладных расходов на Spark.

В обоих сценариях пользователь в итоге использует не тот инструмент, который нужен для работы. Этих сценариев можно избежать, если разделить логику и исполнение. **Использование Fugue в качестве уровня абстракции позволяет пользователям писать одну кодовую базу, совместимую как с Pandas, так и со Spark.** Фреймворк, на котором исполняется код, может быть уточнен во время выполнения программы путем указания конкретного движка. Чтобы продемонстрировать это, давайте рассмотрим самый простой способ использования Fugue - функцию `transform()`.

Для данного примера у нас есть DataFrame со столбцами `id` и `value`. Мы хотим создать столбец под названием `food` путем сопоставления `value` с соответствующим `food` в словаре `mapping`.

```python
import pandas as pd
from typing import Dict

input_df = pd.DataFrame({"id":[0,1,2], "value": (["A", "B", "C"])})
mapping = {"A": "Apple", "B": "Banana", "C": "Carrot"}
```

В Pandas есть простой метод для этого. Мы можем создать функцию Pandas, которая вызывает его.

```python
def map_letter_to_food(df: pd.DataFrame, mapping: Dict) -> pd.DataFrame:
    df["food"] = df["value"].map(mapping)
    return df
```

Не редактируя функцию Pandas, мы можем перенести ее в Spark с помощью функции `transform()` в Fugue. Эта функция может принимать Pandas DataFrame или Spark DataFrame, возвращает же она Spark DataFrame (если в аргументе engine указан движок Spark).

```python
from fugue import transform
from pyspark.sql import SparkSession

spark_session = SparkSession.builder.getOrCreate()

df = transform(input_df,
               map_letter_to_food,
               schema="*, food:str",
               params=dict(mapping=mapping),
               engine=spark_session
               )
df.show()
```

Обратите внимание, что нам нужно вызвать `.show()`, поскольку Spark выполняет расчет “лениво”. Вывод показан ниже.

![spark_show1.png](assets/spark_show1.png)

В этом сценарии нам не пришлось редактировать исходную функцию на базе Pandas. Функция `transform()` позаботилась о переносе исполнения кода на Spark, поскольку в качестве движка мы указали `spark_session`. Если движок не указан, по умолчанию используется движок на основе Pandas. Пользователям Pandas может быть не привычно каждый раз явно определять `schema`, но таково требование для распределенных вычислений.

Однако на самом деле Pandas не всегда будет самым простым способом выражения логики. Таким образом, Fugue также поддерживает использование нативных функций Python, будучи **гибким в обработке различных типов ввода и вывода.** Ниже приведены три различные реализации для нашей функции `map_letter_to_food()`. Все они совместимы с функцией Fugue `transform()` и могут быть использованы на движках Pandas, Spark и Dask с одинаковым синтаксисом.

```python
from typing import List, Dict, Any, Iterable

def map_letter_to_food2(df: List[Dict[str,Any]], mapping: Dict) -> Iterable[Dict[str,Any]]:
    for row in df:
        row["food"] = mapping[row["value"]]
        yield row

def map_letter_to_food3(df: List[List[Any]], mapping: Dict) -> List[List[Any]]:
    for row in df:
        row.append(mapping[row[1]])
    return df

def map_letter_to_food4(df: List[List[Any]], mapping: Dict) -> pd.DataFrame:
    for row in df:
        row.append(mapping[row[1]])
    df = pd.DataFrame.from_records(df, columns=["id", "value", "food"])
    return df
```

Обратите внимание, что вся логика определена в функции `map_letter_to_food()`. Выполнение откладывается до вызова `transform()`, где мы указываем движок. **Пользователям нужно только определить свою логику удобным для них способом.** Затем Fugue выполнит работу по доведению ее до указанного движка.

В то время как Spark предоставляет `Pandas API` в качестве способа выполнения функций Pandas на Spark, Fugue предоставляет более простой интерфейс, сосредоточенный вокруг определеннной schema. [Здесь](https://fugue-tutorials.readthedocs.io/tutorials/beginner/schema.html) schema передается в `transform()` в виде единственной строки, оставляя исходную функцию нетронутой.

На более практическом уровне очень часто команды специалистов по анализу данных имеют общие библиотеки, содержащие специфическую бизнес-логику для очистки и преобразования данных. В настоящее время эту логику приходится реализовывать дважды - один раз для проектов на Pandas и второй раз для проектов на Spark). С помощью Fugue **одну и ту же функцию можно использовать как на движке Pandas, так и на движке Spark без изменения кода.**

Это также делает код **перспективным в будущем.** Что если однажды вы решите, что хотите использовать движок Dask? Что если вы захотите использовать движок Ray? Использование Fugue в качестве слоя абстракции позволит вам легко мигрировать, поскольку это будет просто вопрос указания движка во время исполнения. С другой стороны, написание кода с использованием Spark автоматически привяжет кодовую базу к этому фреймворку. Минималистичный интерфейс Fugue намеренно упрощает процесс перехода между фреймворками, если пользователь этого захочет.

## Улучшение тестируемости Spark
*Как мы можем ускорить процесс разработки и тестирование в проектах больших данных?*
