{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BylxOfcDa_XK"
   },
   "outputs": [],
   "source": [
    "# ! pip install pyspark\n",
    "# ! pip install fugue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mJTIU-syap5p"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "input_df = pd.DataFrame({\"id\":[0,1,2], \"value\": ([\"A\", \"B\", \"C\"])})\n",
    "mapping = {\"A\": \"Apple\", \"B\": \"Banana\", \"C\": \"Carrot\"}\n",
    "\n",
    "def map_letter_to_food(df: pd.DataFrame, mapping: Dict) -> pd.DataFrame:\n",
    "    df[\"food\"] = df[\"value\"].map(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcaqUkPIbMPN",
    "outputId": "3d1a73a1-ad35-41c7-ad06-b34b1aabe9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id|value|  food|\n",
      "+---+-----+------+\n",
      "|  0|    A| Apple|\n",
      "|  1|    B|Banana|\n",
      "|  2|    C|Carrot|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import transform\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = transform(input_df,\n",
    "               map_letter_to_food,\n",
    "               schema=\"*, food:str\",\n",
    "               params=dict(mapping=mapping),\n",
    "               engine=spark_session\n",
    "               )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vKT1XumLbn19"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Iterable\n",
    "\n",
    "def map_letter_to_food2(df: List[Dict[str,Any]], mapping: Dict) -> Iterable[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        row[\"food\"] = mapping[row[\"value\"]]\n",
    "        yield row\n",
    "\n",
    "def map_letter_to_food3(df: List[List[Any]], mapping: Dict) -> List[List[Any]]:\n",
    "    for row in df:\n",
    "        row.append(mapping[row[1]])\n",
    "    return df\n",
    "\n",
    "def map_letter_to_food4(df: List[List[Any]], mapping: Dict) -> pd.DataFrame:\n",
    "    for row in df:\n",
    "        row.append(mapping[row[1]])\n",
    "    df = pd.DataFrame.from_records(df, columns=[\"id\", \"value\", \"food\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIxWdjbxfXRB",
    "outputId": "389971d4-d207-4262-f5c5-b4ba5b466de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "col1:long|col2:long|col3:int\n",
      "---------+---------+--------\n",
      "1        |2        |3       \n",
      "2        |3        |5       \n",
      "3        |4        |7       \n",
      "Total count: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from fugue.workflow import FugueWorkflow\n",
    "\n",
    "data = pd.DataFrame({'col1': [1,2,3], 'col2':[2,3,4]})\n",
    "\n",
    "def make_new_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['col3'] = df['col1'] + df['col2']\n",
    "    return df\n",
    "\n",
    "spark_session = SparkSession.builder.getOrCreate()\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df(data)\n",
    "df = df.transform(make_new_col, schema=\"*, col3:int\")\n",
    "dag.run(spark_session)\n",
    "df.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBpI92_DT5kJ",
    "outputId": "e862b903-f0b9-439f-a1b9-aade40563c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "col1:long|col2:long|col3:int\n",
      "---------+---------+--------\n",
      "1        |2        |3       \n",
      "2        |3        |5       \n",
      "3        |4        |7       \n",
      "Total count: 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FugueWorkflowResult()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fugue import fsql\n",
    "\n",
    "fsql(\"\"\"SELECT col1, col2 \n",
    "          FROM data \n",
    "     TRANSFORM USING make_new_col SCHEMA *,col3:int \n",
    "         PRINT\"\"\").run(spark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partition_by",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m dag \u001b[38;5;241m=\u001b[39m FugueWorkflow()\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m dag\u001b[38;5;241m.\u001b[39mdf(df)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mdag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition_by\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, presort\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol2 desc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      9\u001b[0m dag\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3_new/lib/python3.9/site-packages/fugue/workflow/workflow.py:2212\u001b[0m, in \u001b[0;36mFugueWorkflow.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     \u001b[38;5;124;03m\"\"\"The dummy method to avoid PyLint complaint\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: partition_by"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'col1': [3, 4, 2, 6, 1, 7, 2],\n",
    "    'col2': [6, 5, 2, 3, 8, 9, 5]\n",
    "})\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df(df)\n",
    "dag.(\"col\", presort=\"col2 desc\").take(5)\n",
    "dag.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPXCV4hCXyPDitwDfWAIY1l",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
