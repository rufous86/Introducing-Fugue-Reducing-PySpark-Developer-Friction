{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXCV4hCXyPDitwDfWAIY1l"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ! pip install pyspark\n",
        "# ! pip install fugue"
      ],
      "metadata": {
        "id": "BylxOfcDa_XK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mJTIU-syap5p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import Dict\n",
        "\n",
        "input_df = pd.DataFrame({\"id\":[0,1,2], \"value\": ([\"A\", \"B\", \"C\"])})\n",
        "mapping = {\"A\": \"Apple\", \"B\": \"Banana\", \"C\": \"Carrot\"}\n",
        "\n",
        "def map_letter_to_food(df: pd.DataFrame, mapping: Dict) -> pd.DataFrame:\n",
        "    df[\"food\"] = df[\"value\"].map(mapping)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fugue import transform\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark_session = SparkSession.builder.getOrCreate()\n",
        "\n",
        "df = transform(input_df,\n",
        "               map_letter_to_food,\n",
        "               schema=\"*, food:str\",\n",
        "               params=dict(mapping=mapping),\n",
        "               engine=spark_session\n",
        "               )\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcaqUkPIbMPN",
        "outputId": "3d1a73a1-ad35-41c7-ad06-b34b1aabe9ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+\n",
            "| id|value|  food|\n",
            "+---+-----+------+\n",
            "|  0|    A| Apple|\n",
            "|  1|    B|Banana|\n",
            "|  2|    C|Carrot|\n",
            "+---+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any, Iterable\n",
        "\n",
        "def map_letter_to_food2(df: List[Dict[str,Any]], mapping: Dict) -> Iterable[Dict[str,Any]]:\n",
        "    for row in df:\n",
        "        row[\"food\"] = mapping[row[\"value\"]]\n",
        "        yield row\n",
        "\n",
        "def map_letter_to_food3(df: List[List[Any]], mapping: Dict) -> List[List[Any]]:\n",
        "    for row in df:\n",
        "        row.append(mapping[row[1]])\n",
        "    return df\n",
        "\n",
        "def map_letter_to_food4(df: List[List[Any]], mapping: Dict) -> pd.DataFrame:\n",
        "    for row in df:\n",
        "        row.append(mapping[row[1]])\n",
        "    df = pd.DataFrame.from_records(df, columns=[\"id\", \"value\", \"food\"])\n",
        "    return df"
      ],
      "metadata": {
        "id": "vKT1XumLbn19"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from fugue.workflow import FugueWorkflow\n",
        "\n",
        "data = pd.DataFrame({'col1': [1,2,3], 'col2':[2,3,4]})\n",
        "\n",
        "def make_new_col(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df['col3'] = df['col1'] + df['col2']\n",
        "    return df\n",
        "\n",
        "spark_session = SparkSession.builder.getOrCreate()\n",
        "dag = FugueWorkflow()\n",
        "df = dag.df(data)\n",
        "df = df.transform(make_new_col, schema=\"*, col3:int\")\n",
        "dag.run(spark_session)\n",
        "df.result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIxWdjbxfXRB",
        "outputId": "389971d4-d207-4262-f5c5-b4ba5b466de1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkDataFrame\n",
            "col1:long|col2:long|col3:int\n",
            "---------+---------+--------\n",
            "1        |2        |3       \n",
            "2        |3        |5       \n",
            "3        |4        |7       \n",
            "Total count: 3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fugue import fsql\n",
        "\n",
        "fsql(\"\"\"SELECT col1, col2 \n",
        "          FROM data \n",
        "     TRANSFORM USING make_new_col SCHEMA *,col3:int \n",
        "         PRINT\"\"\").run(spark_session)"
      ],
      "metadata": {
        "id": "vBpI92_DT5kJ",
        "outputId": "e862b903-f0b9-439f-a1b9-aade40563c3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkDataFrame\n",
            "col1:long|col2:long|col3:int\n",
            "---------+---------+--------\n",
            "1        |2        |3       \n",
            "2        |3        |5       \n",
            "3        |4        |7       \n",
            "Total count: 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FugueWorkflowResult()"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}